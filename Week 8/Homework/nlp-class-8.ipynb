{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clean everything","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\ncurrent_folder = os.getcwd()\n\nfor item in os.listdir(current_folder):\n    item_path = os.path.join(current_folder, item)\n    \n    if os.path.isfile(item_path) or os.path.islink(item_path):\n        os.remove(item_path)\n    elif os.path.isdir(item_path):\n        shutil.rmtree(item_path)\n\nprint(\"All files and folders have been removed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:05.463835Z","iopub.execute_input":"2025-03-09T10:59:05.464292Z","execution_failed":"2025-03-09T12:10:25.643Z"}},"outputs":[{"name":"stdout","text":"All files and folders have been removed!\nAll files and folders have been removed!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Download data","metadata":{}},{"cell_type":"code","source":"!curl -L -o {curr_path}/dataset.zip https://www.kaggle.com/competitions/digit-recognizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:05.473037Z","iopub.execute_input":"2025-03-09T10:59:05.473316Z","iopub.status.idle":"2025-03-09T10:59:05.945945Z","shell.execute_reply.started":"2025-03-09T10:59:05.473283Z","shell.execute_reply":"2025-03-09T10:59:05.945211Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file {curr_path}/dataset.zip: No such file or \nWarning: directory\n100  1369    0  1369    0     0   5020      0 --:--:-- --:--:-- --:--:--  5033\ncurl: (23) Failure writing output to destination\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:05.947576Z","iopub.execute_input":"2025-03-09T10:59:05.947803Z","iopub.status.idle":"2025-03-09T10:59:08.395466Z","shell.execute_reply.started":"2025-03-09T10:59:05.947776Z","shell.execute_reply":"2025-03-09T10:59:08.394797Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"curr_path = os.getcwd()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:08.396796Z","iopub.execute_input":"2025-03-09T10:59:08.397292Z","iopub.status.idle":"2025-03-09T10:59:08.400582Z","shell.execute_reply.started":"2025-03-09T10:59:08.397256Z","shell.execute_reply":"2025-03-09T10:59:08.399791Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# os.makedirs(f\"{curr_path}/dataset\", exist_ok = True)\n# if \"dataset.zip\" in os.listdir(f\"{curr_path}/\"):\n#     shutil.move(f\"{curr_path}/dataset.zip\", f\"{curr_path}/dataset/dataset.zip\")\n# temp = f\"{curr_path}/dataset/dataset.zip\"\n# des = f\"{curr_path}/dataset/\"\n# if not os.path.exists(temp):\n#     print(\"Error: ZIP file not found!\")\n# with zipfile.ZipFile(\"/kaggle/working/dataset/dataset.zip\", 'r') as zip_ref:\n#     zip_ref.extractall(des)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:08.401358Z","iopub.execute_input":"2025-03-09T10:59:08.401694Z","iopub.status.idle":"2025-03-09T10:59:08.416076Z","shell.execute_reply.started":"2025-03-09T10:59:08.401672Z","shell.execute_reply":"2025-03-09T10:59:08.415198Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"input_path = \"/kaggle/input/digit-recognizer\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:08.416884Z","iopub.execute_input":"2025-03-09T10:59:08.417195Z","iopub.status.idle":"2025-03-09T10:59:08.432264Z","shell.execute_reply.started":"2025-03-09T10:59:08.417164Z","shell.execute_reply":"2025-03-09T10:59:08.431586Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{input_path}/train.csv\")\ntest_df = pd.read_csv(f\"{input_path}/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:08.433001Z","iopub.execute_input":"2025-03-09T10:59:08.433280Z","iopub.status.idle":"2025-03-09T10:59:12.314971Z","shell.execute_reply.started":"2025-03-09T10:59:08.433261Z","shell.execute_reply":"2025-03-09T10:59:12.314053Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:12.317226Z","iopub.execute_input":"2025-03-09T10:59:12.317448Z","iopub.status.idle":"2025-03-09T10:59:13.903331Z","shell.execute_reply.started":"2025-03-09T10:59:12.317430Z","shell.execute_reply":"2025-03-09T10:59:13.902512Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\ncount  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \nmean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \nstd        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \nmin        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \nmax        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n\n        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\ncount  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \nmean       0.0      0.0      0.0  ...      0.219286      0.117095   \nstd        0.0      0.0      0.0  ...      6.312890      4.633819   \nmin        0.0      0.0      0.0  ...      0.000000      0.000000   \n25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n75%        0.0      0.0      0.0  ...      0.000000      0.000000   \nmax        0.0      0.0      0.0  ...    254.000000    254.000000   \n\n           pixel776     pixel777      pixel778      pixel779  pixel780  \\\ncount  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \nmean       0.059024      0.02019      0.017238      0.002857       0.0   \nstd        3.274488      1.75987      1.894498      0.414264       0.0   \nmin        0.000000      0.00000      0.000000      0.000000       0.0   \n25%        0.000000      0.00000      0.000000      0.000000       0.0   \n50%        0.000000      0.00000      0.000000      0.000000       0.0   \n75%        0.000000      0.00000      0.000000      0.000000       0.0   \nmax      253.000000    253.00000    254.000000     62.000000       0.0   \n\n       pixel781  pixel782  pixel783  \ncount   42000.0   42000.0   42000.0  \nmean        0.0       0.0       0.0  \nstd         0.0       0.0       0.0  \nmin         0.0       0.0       0.0  \n25%         0.0       0.0       0.0  \n50%         0.0       0.0       0.0  \n75%         0.0       0.0       0.0  \nmax         0.0       0.0       0.0  \n\n[8 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>42000.000000</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>...</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.00000</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.456643</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.219286</td>\n      <td>0.117095</td>\n      <td>0.059024</td>\n      <td>0.02019</td>\n      <td>0.017238</td>\n      <td>0.002857</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.887730</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6.312890</td>\n      <td>4.633819</td>\n      <td>3.274488</td>\n      <td>1.75987</td>\n      <td>1.894498</td>\n      <td>0.414264</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>254.000000</td>\n      <td>254.000000</td>\n      <td>253.000000</td>\n      <td>253.00000</td>\n      <td>254.000000</td>\n      <td>62.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_df['label'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:13.904235Z","iopub.execute_input":"2025-03-09T10:59:13.904486Z","iopub.status.idle":"2025-03-09T10:59:13.911861Z","shell.execute_reply.started":"2025-03-09T10:59:13.904467Z","shell.execute_reply":"2025-03-09T10:59:13.911235Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([1, 0, 4, 7, 3, 5, 8, 9, 2, 6])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"markdown","source":"## Model 1","metadata":{}},{"cell_type":"code","source":"# # Preprocess data\n# X = train_df.iloc[:, 1:].values / 255.0  # Normalize pixel values\n# y = train_df.iloc[:, 0].values           # Labels\n\n# # Train-test split\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Convert to PyTorch tensors\n# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n# y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# # Create DataLoaders\n# batch_size = 64\n# train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n# val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n\n# # Define the MLP model using nn.Sequential\n# class MLPModel(nn.Module):\n#     def __init__(self, input_size=784, hidden_size=256, output_size=10):\n#         super(MLPModel, self).__init__()\n        \n#         self.model = nn.Sequential(\n#             nn.Linear(input_size, hidden_size),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n\n#             nn.Linear(hidden_size, hidden_size),\n#             nn.ReLU(),\n#             nn.Dropout(0.3),\n\n#             nn.Linear(hidden_size, output_size)\n#         )\n\n#     def forward(self, x):\n#         return self.model(x)\n\n# # Initialize model, loss function, and optimizer\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = MLPModel().to(device)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# model_path = \"a1.pth\"\n\n# # Training loop\n# num_epochs = 300\n# best_loss = float('inf')\n# for epoch in range(num_epochs):\n#     model.train()\n#     running_loss = 0.0\n    \n#     for inputs, labels in train_loader:\n#         inputs, labels = inputs.to(device), labels.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n\n#     curr_loss = running_loss/len(train_loader)\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {curr_loss:.4f}\")\n\n#     # Check if validation loss improved\n#     if curr_loss < best_loss:\n#         best_loss = curr_loss\n#         torch.save(model.state_dict(), model_path)\n#         print(f\"✅ Epoch {epoch+1}: Validation loss improved to {curr_loss:.4f}. Model saved!\")\n#         epochs_no_improve = 0\n#     else:\n#         epochs_no_improve += 1\n#         print(f\"⚠️ No improvement in validation loss for {epochs_no_improve} epochs.\")\n\n# # Validation\n# model.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for inputs, labels in val_loader:\n#         inputs, labels = inputs.to(device), labels.to(device)\n#         outputs = model(inputs)\n#         _, predicted = torch.max(outputs, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n\n# print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T10:59:13.912423Z","iopub.execute_input":"2025-03-09T10:59:13.912674Z","iopub.status.idle":"2025-03-09T10:59:13.927090Z","shell.execute_reply.started":"2025-03-09T10:59:13.912654Z","shell.execute_reply":"2025-03-09T10:59:13.926319Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Model 2","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n# Preprocess data\nX = train_df.iloc[:, 1:].values / 255.0  # Normalize pixel values\ny = train_df.iloc[:, 0].values           # Labels\n\n# Train-validation split (80-20)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n\n# Define the MLP model using nn.Sequential\nclass MLPModel(nn.Module):\n    def __init__(self, input_size=784, hidden_size=[512,256,128,64], output_size=10):\n        super(MLPModel, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(input_size, hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n           \n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            nn.Tanh(),\n            # nn.Sigmoid(),\n            nn.Dropout(0.3),\n\n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            nn.LeakyReLU(negative_slope=0.03),\n            # nn.Tanh(),\n            # nn.Sigmoid(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            nn.ReLU(),\n            # nn.Mish(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            # nn.Tanh(),\n            # nn.Sigmoid(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(hidden_size[0], output_size)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialize model, loss function, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLPModel().to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n# optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\noptimizer = optim.RAdam(model.parameters(), lr=0.001, betas=(0.9, 0.9999), weight_decay=1e-4)\n# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=True)\n\n\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=5)\n# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=num_epochs)\n\n# Training loop with best model saving\nnum_epochs = 300\npatience = 200  # Number of epochs to wait before stopping\nbest_val_loss = float(\"inf\")  # Start with a very high loss\nbest_val_acc = 0.0\nmodel_path = \"best_mlp_digit_recognizer.pth\"\nepochs_no_improve = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    avg_train_loss = running_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n\n    current_lr = optimizer.param_groups[0]['lr']\n    scheduler.step(avg_val_loss)\n    new_lr = optimizer.param_groups[0]['lr']\n    if new_lr < current_lr:\n        print(f\"📘 Learning rate reduced from {current_lr:.6f} to {new_lr:.6f}\")\n    \n    # Check if validation loss improved\n    if val_accuracy > best_val_acc:\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy \n        torch.save(model.state_dict(), model_path)\n        print(f\"✅ Epoch {epoch+1}: Validation loss improved to {best_val_loss:.4f}. Model saved!\")\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        print(f\"⚠️ No improvement in validation loss for {epochs_no_improve} epochs.\")\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n    # Stop training if no improvement for 'patience' epochs\n    if epochs_no_improve >= patience:\n        print(\"🚨 Early stopping triggered! Loading best model...\")\n        break\n\n# Load the best model\nmodel.load_state_dict(torch.load(model_path))\nprint(f\"🎯 Best model loaded with Val Loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:20:20.693152Z","iopub.execute_input":"2025-03-09T11:20:20.693507Z","iopub.status.idle":"2025-03-09T11:31:17.166029Z","shell.execute_reply.started":"2025-03-09T11:20:20.693482Z","shell.execute_reply":"2025-03-09T11:31:17.165100Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n✅ Epoch 1: Validation loss improved to 0.7135. Model saved!\nEpoch [1/300], Train Loss: 1.1828, Val Loss: 0.7135, Val Acc: 93.39%\n✅ Epoch 2: Validation loss improved to 0.6575. Model saved!\nEpoch [2/300], Train Loss: 0.7458, Val Loss: 0.6575, Val Acc: 95.27%\n✅ Epoch 3: Validation loss improved to 0.6258. Model saved!\nEpoch [3/300], Train Loss: 0.6845, Val Loss: 0.6258, Val Acc: 96.18%\n✅ Epoch 4: Validation loss improved to 0.6035. Model saved!\nEpoch [4/300], Train Loss: 0.6554, Val Loss: 0.6035, Val Acc: 96.74%\n✅ Epoch 5: Validation loss improved to 0.5897. Model saved!\nEpoch [5/300], Train Loss: 0.6335, Val Loss: 0.5897, Val Acc: 97.18%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [6/300], Train Loss: 0.6181, Val Loss: 0.5874, Val Acc: 97.15%\n✅ Epoch 7: Validation loss improved to 0.5818. Model saved!\nEpoch [7/300], Train Loss: 0.6062, Val Loss: 0.5818, Val Acc: 97.20%\n✅ Epoch 8: Validation loss improved to 0.5709. Model saved!\nEpoch [8/300], Train Loss: 0.5981, Val Loss: 0.5709, Val Acc: 97.39%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [9/300], Train Loss: 0.5962, Val Loss: 0.5715, Val Acc: 97.38%\n✅ Epoch 10: Validation loss improved to 0.5664. Model saved!\nEpoch [10/300], Train Loss: 0.5911, Val Loss: 0.5664, Val Acc: 97.54%\n✅ Epoch 11: Validation loss improved to 0.5663. Model saved!\nEpoch [11/300], Train Loss: 0.5841, Val Loss: 0.5663, Val Acc: 97.65%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [12/300], Train Loss: 0.5820, Val Loss: 0.5655, Val Acc: 97.63%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [13/300], Train Loss: 0.5780, Val Loss: 0.5667, Val Acc: 97.49%\n✅ Epoch 14: Validation loss improved to 0.5609. Model saved!\nEpoch [14/300], Train Loss: 0.5749, Val Loss: 0.5609, Val Acc: 97.70%\n✅ Epoch 15: Validation loss improved to 0.5609. Model saved!\nEpoch [15/300], Train Loss: 0.5721, Val Loss: 0.5640, Val Acc: 97.74%\n✅ Epoch 16: Validation loss improved to 0.5597. Model saved!\nEpoch [16/300], Train Loss: 0.5717, Val Loss: 0.5597, Val Acc: 97.81%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [17/300], Train Loss: 0.5693, Val Loss: 0.5579, Val Acc: 97.77%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [18/300], Train Loss: 0.5686, Val Loss: 0.5596, Val Acc: 97.81%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [19/300], Train Loss: 0.5682, Val Loss: 0.5618, Val Acc: 97.73%\n✅ Epoch 20: Validation loss improved to 0.5587. Model saved!\nEpoch [20/300], Train Loss: 0.5680, Val Loss: 0.5587, Val Acc: 97.96%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [21/300], Train Loss: 0.5629, Val Loss: 0.5586, Val Acc: 97.83%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [22/300], Train Loss: 0.5649, Val Loss: 0.5596, Val Acc: 97.92%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [23/300], Train Loss: 0.5638, Val Loss: 0.5576, Val Acc: 97.73%\n⚠️ No improvement in validation loss for 4 epochs.\nEpoch [24/300], Train Loss: 0.5644, Val Loss: 0.5590, Val Acc: 97.67%\n⚠️ No improvement in validation loss for 5 epochs.\nEpoch [25/300], Train Loss: 0.5654, Val Loss: 0.5579, Val Acc: 97.75%\n⚠️ No improvement in validation loss for 6 epochs.\nEpoch [26/300], Train Loss: 0.5619, Val Loss: 0.5615, Val Acc: 97.68%\n⚠️ No improvement in validation loss for 7 epochs.\nEpoch [27/300], Train Loss: 0.5633, Val Loss: 0.5641, Val Acc: 97.48%\n⚠️ No improvement in validation loss for 8 epochs.\nEpoch [28/300], Train Loss: 0.5603, Val Loss: 0.5593, Val Acc: 97.80%\n📘 Learning rate reduced from 0.001000 to 0.000800\n⚠️ No improvement in validation loss for 9 epochs.\nEpoch [29/300], Train Loss: 0.5642, Val Loss: 0.5603, Val Acc: 97.71%\n⚠️ No improvement in validation loss for 10 epochs.\nEpoch [30/300], Train Loss: 0.5551, Val Loss: 0.5582, Val Acc: 97.65%\n✅ Epoch 31: Validation loss improved to 0.5535. Model saved!\nEpoch [31/300], Train Loss: 0.5550, Val Loss: 0.5535, Val Acc: 98.00%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [32/300], Train Loss: 0.5535, Val Loss: 0.5556, Val Acc: 97.96%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [33/300], Train Loss: 0.5591, Val Loss: 0.5592, Val Acc: 97.76%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [34/300], Train Loss: 0.5556, Val Loss: 0.5572, Val Acc: 97.85%\n⚠️ No improvement in validation loss for 4 epochs.\nEpoch [35/300], Train Loss: 0.5572, Val Loss: 0.5570, Val Acc: 97.89%\n⚠️ No improvement in validation loss for 5 epochs.\nEpoch [36/300], Train Loss: 0.5543, Val Loss: 0.5597, Val Acc: 97.69%\n📘 Learning rate reduced from 0.000800 to 0.000640\n⚠️ No improvement in validation loss for 6 epochs.\nEpoch [37/300], Train Loss: 0.5564, Val Loss: 0.5582, Val Acc: 97.80%\n⚠️ No improvement in validation loss for 7 epochs.\nEpoch [38/300], Train Loss: 0.5497, Val Loss: 0.5540, Val Acc: 97.87%\n⚠️ No improvement in validation loss for 8 epochs.\nEpoch [39/300], Train Loss: 0.5495, Val Loss: 0.5540, Val Acc: 97.90%\n⚠️ No improvement in validation loss for 9 epochs.\nEpoch [40/300], Train Loss: 0.5496, Val Loss: 0.5540, Val Acc: 97.85%\n⚠️ No improvement in validation loss for 10 epochs.\nEpoch [41/300], Train Loss: 0.5491, Val Loss: 0.5551, Val Acc: 97.83%\n⚠️ No improvement in validation loss for 11 epochs.\nEpoch [42/300], Train Loss: 0.5531, Val Loss: 0.5572, Val Acc: 97.88%\n📘 Learning rate reduced from 0.000640 to 0.000512\n⚠️ No improvement in validation loss for 12 epochs.\nEpoch [43/300], Train Loss: 0.5525, Val Loss: 0.5552, Val Acc: 97.95%\n⚠️ No improvement in validation loss for 13 epochs.\nEpoch [44/300], Train Loss: 0.5470, Val Loss: 0.5517, Val Acc: 97.92%\n✅ Epoch 45: Validation loss improved to 0.5486. Model saved!\nEpoch [45/300], Train Loss: 0.5438, Val Loss: 0.5486, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [46/300], Train Loss: 0.5467, Val Loss: 0.5531, Val Acc: 97.95%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [47/300], Train Loss: 0.5469, Val Loss: 0.5507, Val Acc: 98.02%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [48/300], Train Loss: 0.5451, Val Loss: 0.5524, Val Acc: 98.00%\n⚠️ No improvement in validation loss for 4 epochs.\nEpoch [49/300], Train Loss: 0.5483, Val Loss: 0.5592, Val Acc: 97.80%\n⚠️ No improvement in validation loss for 5 epochs.\nEpoch [50/300], Train Loss: 0.5485, Val Loss: 0.5505, Val Acc: 98.06%\n📘 Learning rate reduced from 0.000512 to 0.000410\n⚠️ No improvement in validation loss for 6 epochs.\nEpoch [51/300], Train Loss: 0.5474, Val Loss: 0.5535, Val Acc: 97.98%\n⚠️ No improvement in validation loss for 7 epochs.\nEpoch [52/300], Train Loss: 0.5443, Val Loss: 0.5523, Val Acc: 97.99%\n⚠️ No improvement in validation loss for 8 epochs.\nEpoch [53/300], Train Loss: 0.5424, Val Loss: 0.5480, Val Acc: 98.07%\n✅ Epoch 54: Validation loss improved to 0.5477. Model saved!\nEpoch [54/300], Train Loss: 0.5432, Val Loss: 0.5477, Val Acc: 98.32%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [55/300], Train Loss: 0.5433, Val Loss: 0.5499, Val Acc: 97.98%\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [56/300], Train Loss: 0.5429, Val Loss: 0.5502, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [57/300], Train Loss: 0.5431, Val Loss: 0.5489, Val Acc: 98.11%\n⚠️ No improvement in validation loss for 4 epochs.\nEpoch [58/300], Train Loss: 0.5433, Val Loss: 0.5517, Val Acc: 98.00%\n⚠️ No improvement in validation loss for 5 epochs.\nEpoch [59/300], Train Loss: 0.5459, Val Loss: 0.5495, Val Acc: 98.07%\n📘 Learning rate reduced from 0.000410 to 0.000328\n⚠️ No improvement in validation loss for 6 epochs.\nEpoch [60/300], Train Loss: 0.5440, Val Loss: 0.5518, Val Acc: 98.02%\n⚠️ No improvement in validation loss for 7 epochs.\nEpoch [61/300], Train Loss: 0.5396, Val Loss: 0.5465, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 8 epochs.\nEpoch [62/300], Train Loss: 0.5393, Val Loss: 0.5503, Val Acc: 97.90%\n⚠️ No improvement in validation loss for 9 epochs.\nEpoch [63/300], Train Loss: 0.5407, Val Loss: 0.5505, Val Acc: 98.02%\n⚠️ No improvement in validation loss for 10 epochs.\nEpoch [64/300], Train Loss: 0.5403, Val Loss: 0.5508, Val Acc: 98.02%\n⚠️ No improvement in validation loss for 11 epochs.\nEpoch [65/300], Train Loss: 0.5392, Val Loss: 0.5483, Val Acc: 98.04%\n⚠️ No improvement in validation loss for 12 epochs.\nEpoch [66/300], Train Loss: 0.5412, Val Loss: 0.5491, Val Acc: 98.07%\n📘 Learning rate reduced from 0.000328 to 0.000262\n⚠️ No improvement in validation loss for 13 epochs.\nEpoch [67/300], Train Loss: 0.5415, Val Loss: 0.5506, Val Acc: 97.92%\n⚠️ No improvement in validation loss for 14 epochs.\nEpoch [68/300], Train Loss: 0.5368, Val Loss: 0.5487, Val Acc: 98.07%\n⚠️ No improvement in validation loss for 15 epochs.\nEpoch [69/300], Train Loss: 0.5366, Val Loss: 0.5481, Val Acc: 98.05%\n⚠️ No improvement in validation loss for 16 epochs.\nEpoch [70/300], Train Loss: 0.5367, Val Loss: 0.5475, Val Acc: 98.01%\n⚠️ No improvement in validation loss for 17 epochs.\nEpoch [71/300], Train Loss: 0.5362, Val Loss: 0.5512, Val Acc: 98.00%\n⚠️ No improvement in validation loss for 18 epochs.\nEpoch [72/300], Train Loss: 0.5370, Val Loss: 0.5490, Val Acc: 97.99%\n📘 Learning rate reduced from 0.000262 to 0.000210\n⚠️ No improvement in validation loss for 19 epochs.\nEpoch [73/300], Train Loss: 0.5369, Val Loss: 0.5494, Val Acc: 98.01%\n⚠️ No improvement in validation loss for 20 epochs.\nEpoch [74/300], Train Loss: 0.5337, Val Loss: 0.5445, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 21 epochs.\nEpoch [75/300], Train Loss: 0.5343, Val Loss: 0.5450, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 22 epochs.\nEpoch [76/300], Train Loss: 0.5346, Val Loss: 0.5453, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 23 epochs.\nEpoch [77/300], Train Loss: 0.5335, Val Loss: 0.5453, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 24 epochs.\nEpoch [78/300], Train Loss: 0.5331, Val Loss: 0.5453, Val Acc: 98.01%\n⚠️ No improvement in validation loss for 25 epochs.\nEpoch [79/300], Train Loss: 0.5328, Val Loss: 0.5470, Val Acc: 98.18%\n📘 Learning rate reduced from 0.000210 to 0.000168\n⚠️ No improvement in validation loss for 26 epochs.\nEpoch [80/300], Train Loss: 0.5333, Val Loss: 0.5455, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 27 epochs.\nEpoch [81/300], Train Loss: 0.5310, Val Loss: 0.5444, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 28 epochs.\nEpoch [82/300], Train Loss: 0.5301, Val Loss: 0.5456, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 29 epochs.\nEpoch [83/300], Train Loss: 0.5322, Val Loss: 0.5441, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 30 epochs.\nEpoch [84/300], Train Loss: 0.5321, Val Loss: 0.5443, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 31 epochs.\nEpoch [85/300], Train Loss: 0.5323, Val Loss: 0.5446, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 32 epochs.\nEpoch [86/300], Train Loss: 0.5302, Val Loss: 0.5436, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 33 epochs.\nEpoch [87/300], Train Loss: 0.5319, Val Loss: 0.5465, Val Acc: 98.06%\n⚠️ No improvement in validation loss for 34 epochs.\nEpoch [88/300], Train Loss: 0.5326, Val Loss: 0.5457, Val Acc: 98.06%\n⚠️ No improvement in validation loss for 35 epochs.\nEpoch [89/300], Train Loss: 0.5309, Val Loss: 0.5448, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 36 epochs.\nEpoch [90/300], Train Loss: 0.5305, Val Loss: 0.5456, Val Acc: 98.12%\n⚠️ No improvement in validation loss for 37 epochs.\nEpoch [91/300], Train Loss: 0.5316, Val Loss: 0.5450, Val Acc: 98.14%\n📘 Learning rate reduced from 0.000168 to 0.000134\n⚠️ No improvement in validation loss for 38 epochs.\nEpoch [92/300], Train Loss: 0.5306, Val Loss: 0.5440, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 39 epochs.\nEpoch [93/300], Train Loss: 0.5287, Val Loss: 0.5454, Val Acc: 98.04%\n⚠️ No improvement in validation loss for 40 epochs.\nEpoch [94/300], Train Loss: 0.5300, Val Loss: 0.5440, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 41 epochs.\nEpoch [95/300], Train Loss: 0.5288, Val Loss: 0.5434, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 42 epochs.\nEpoch [96/300], Train Loss: 0.5289, Val Loss: 0.5459, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 43 epochs.\nEpoch [97/300], Train Loss: 0.5309, Val Loss: 0.5444, Val Acc: 98.12%\n⚠️ No improvement in validation loss for 44 epochs.\nEpoch [98/300], Train Loss: 0.5294, Val Loss: 0.5453, Val Acc: 98.11%\n⚠️ No improvement in validation loss for 45 epochs.\nEpoch [99/300], Train Loss: 0.5280, Val Loss: 0.5446, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 46 epochs.\nEpoch [100/300], Train Loss: 0.5297, Val Loss: 0.5433, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 47 epochs.\nEpoch [101/300], Train Loss: 0.5284, Val Loss: 0.5453, Val Acc: 98.10%\n⚠️ No improvement in validation loss for 48 epochs.\nEpoch [102/300], Train Loss: 0.5288, Val Loss: 0.5455, Val Acc: 98.12%\n⚠️ No improvement in validation loss for 49 epochs.\nEpoch [103/300], Train Loss: 0.5292, Val Loss: 0.5463, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 50 epochs.\nEpoch [104/300], Train Loss: 0.5298, Val Loss: 0.5446, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 51 epochs.\nEpoch [105/300], Train Loss: 0.5294, Val Loss: 0.5467, Val Acc: 98.17%\n📘 Learning rate reduced from 0.000134 to 0.000107\n⚠️ No improvement in validation loss for 52 epochs.\nEpoch [106/300], Train Loss: 0.5276, Val Loss: 0.5439, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 53 epochs.\nEpoch [107/300], Train Loss: 0.5269, Val Loss: 0.5434, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 54 epochs.\nEpoch [108/300], Train Loss: 0.5272, Val Loss: 0.5419, Val Acc: 98.32%\n⚠️ No improvement in validation loss for 55 epochs.\nEpoch [109/300], Train Loss: 0.5277, Val Loss: 0.5444, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 56 epochs.\nEpoch [110/300], Train Loss: 0.5276, Val Loss: 0.5434, Val Acc: 98.15%\n⚠️ No improvement in validation loss for 57 epochs.\nEpoch [111/300], Train Loss: 0.5273, Val Loss: 0.5454, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 58 epochs.\nEpoch [112/300], Train Loss: 0.5275, Val Loss: 0.5420, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 59 epochs.\nEpoch [113/300], Train Loss: 0.5269, Val Loss: 0.5421, Val Acc: 98.25%\n📘 Learning rate reduced from 0.000107 to 0.000086\n⚠️ No improvement in validation loss for 60 epochs.\nEpoch [114/300], Train Loss: 0.5267, Val Loss: 0.5429, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 61 epochs.\nEpoch [115/300], Train Loss: 0.5260, Val Loss: 0.5428, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 62 epochs.\nEpoch [116/300], Train Loss: 0.5249, Val Loss: 0.5425, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 63 epochs.\nEpoch [117/300], Train Loss: 0.5252, Val Loss: 0.5427, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 64 epochs.\nEpoch [118/300], Train Loss: 0.5243, Val Loss: 0.5420, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 65 epochs.\nEpoch [119/300], Train Loss: 0.5250, Val Loss: 0.5433, Val Acc: 98.13%\n📘 Learning rate reduced from 0.000086 to 0.000069\n⚠️ No improvement in validation loss for 66 epochs.\nEpoch [120/300], Train Loss: 0.5242, Val Loss: 0.5430, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 67 epochs.\nEpoch [121/300], Train Loss: 0.5252, Val Loss: 0.5429, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 68 epochs.\nEpoch [122/300], Train Loss: 0.5240, Val Loss: 0.5417, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 69 epochs.\nEpoch [123/300], Train Loss: 0.5236, Val Loss: 0.5411, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 70 epochs.\nEpoch [124/300], Train Loss: 0.5241, Val Loss: 0.5426, Val Acc: 98.10%\n⚠️ No improvement in validation loss for 71 epochs.\nEpoch [125/300], Train Loss: 0.5239, Val Loss: 0.5440, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 72 epochs.\nEpoch [126/300], Train Loss: 0.5245, Val Loss: 0.5437, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 73 epochs.\nEpoch [127/300], Train Loss: 0.5242, Val Loss: 0.5422, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 74 epochs.\nEpoch [128/300], Train Loss: 0.5239, Val Loss: 0.5421, Val Acc: 98.32%\n📘 Learning rate reduced from 0.000069 to 0.000055\n⚠️ No improvement in validation loss for 75 epochs.\nEpoch [129/300], Train Loss: 0.5230, Val Loss: 0.5415, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 76 epochs.\nEpoch [130/300], Train Loss: 0.5225, Val Loss: 0.5432, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 77 epochs.\nEpoch [131/300], Train Loss: 0.5227, Val Loss: 0.5430, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 78 epochs.\nEpoch [132/300], Train Loss: 0.5232, Val Loss: 0.5423, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 79 epochs.\nEpoch [133/300], Train Loss: 0.5224, Val Loss: 0.5429, Val Acc: 98.12%\n⚠️ No improvement in validation loss for 80 epochs.\nEpoch [134/300], Train Loss: 0.5231, Val Loss: 0.5429, Val Acc: 98.12%\n📘 Learning rate reduced from 0.000055 to 0.000044\n⚠️ No improvement in validation loss for 81 epochs.\nEpoch [135/300], Train Loss: 0.5227, Val Loss: 0.5433, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 82 epochs.\nEpoch [136/300], Train Loss: 0.5218, Val Loss: 0.5419, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 83 epochs.\nEpoch [137/300], Train Loss: 0.5222, Val Loss: 0.5425, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 84 epochs.\nEpoch [138/300], Train Loss: 0.5216, Val Loss: 0.5426, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 85 epochs.\nEpoch [139/300], Train Loss: 0.5220, Val Loss: 0.5414, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 86 epochs.\nEpoch [140/300], Train Loss: 0.5214, Val Loss: 0.5423, Val Acc: 98.25%\n📘 Learning rate reduced from 0.000044 to 0.000035\n⚠️ No improvement in validation loss for 87 epochs.\nEpoch [141/300], Train Loss: 0.5224, Val Loss: 0.5428, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 88 epochs.\nEpoch [142/300], Train Loss: 0.5208, Val Loss: 0.5411, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 89 epochs.\nEpoch [143/300], Train Loss: 0.5207, Val Loss: 0.5412, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 90 epochs.\nEpoch [144/300], Train Loss: 0.5219, Val Loss: 0.5427, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 91 epochs.\nEpoch [145/300], Train Loss: 0.5212, Val Loss: 0.5416, Val Acc: 98.12%\n⚠️ No improvement in validation loss for 92 epochs.\nEpoch [146/300], Train Loss: 0.5209, Val Loss: 0.5409, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 93 epochs.\nEpoch [147/300], Train Loss: 0.5206, Val Loss: 0.5424, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 94 epochs.\nEpoch [148/300], Train Loss: 0.5211, Val Loss: 0.5423, Val Acc: 98.15%\n⚠️ No improvement in validation loss for 95 epochs.\nEpoch [149/300], Train Loss: 0.5204, Val Loss: 0.5426, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 96 epochs.\nEpoch [150/300], Train Loss: 0.5215, Val Loss: 0.5426, Val Acc: 98.11%\n⚠️ No improvement in validation loss for 97 epochs.\nEpoch [151/300], Train Loss: 0.5208, Val Loss: 0.5424, Val Acc: 98.21%\n📘 Learning rate reduced from 0.000035 to 0.000028\n⚠️ No improvement in validation loss for 98 epochs.\nEpoch [152/300], Train Loss: 0.5209, Val Loss: 0.5430, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 99 epochs.\nEpoch [153/300], Train Loss: 0.5203, Val Loss: 0.5409, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 100 epochs.\nEpoch [154/300], Train Loss: 0.5204, Val Loss: 0.5423, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 101 epochs.\nEpoch [155/300], Train Loss: 0.5203, Val Loss: 0.5407, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 102 epochs.\nEpoch [156/300], Train Loss: 0.5214, Val Loss: 0.5414, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 103 epochs.\nEpoch [157/300], Train Loss: 0.5196, Val Loss: 0.5426, Val Acc: 98.14%\n⚠️ No improvement in validation loss for 104 epochs.\nEpoch [158/300], Train Loss: 0.5205, Val Loss: 0.5422, Val Acc: 98.15%\n⚠️ No improvement in validation loss for 105 epochs.\nEpoch [159/300], Train Loss: 0.5194, Val Loss: 0.5421, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 106 epochs.\nEpoch [160/300], Train Loss: 0.5199, Val Loss: 0.5429, Val Acc: 98.14%\n📘 Learning rate reduced from 0.000028 to 0.000023\n⚠️ No improvement in validation loss for 107 epochs.\nEpoch [161/300], Train Loss: 0.5198, Val Loss: 0.5429, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 108 epochs.\nEpoch [162/300], Train Loss: 0.5199, Val Loss: 0.5424, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 109 epochs.\nEpoch [163/300], Train Loss: 0.5201, Val Loss: 0.5418, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 110 epochs.\nEpoch [164/300], Train Loss: 0.5198, Val Loss: 0.5414, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 111 epochs.\nEpoch [165/300], Train Loss: 0.5200, Val Loss: 0.5420, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 112 epochs.\nEpoch [166/300], Train Loss: 0.5193, Val Loss: 0.5414, Val Acc: 98.24%\n📘 Learning rate reduced from 0.000023 to 0.000018\n⚠️ No improvement in validation loss for 113 epochs.\nEpoch [167/300], Train Loss: 0.5200, Val Loss: 0.5431, Val Acc: 98.13%\n⚠️ No improvement in validation loss for 114 epochs.\nEpoch [168/300], Train Loss: 0.5198, Val Loss: 0.5421, Val Acc: 98.15%\n⚠️ No improvement in validation loss for 115 epochs.\nEpoch [169/300], Train Loss: 0.5191, Val Loss: 0.5421, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 116 epochs.\nEpoch [170/300], Train Loss: 0.5195, Val Loss: 0.5411, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 117 epochs.\nEpoch [171/300], Train Loss: 0.5192, Val Loss: 0.5410, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 118 epochs.\nEpoch [172/300], Train Loss: 0.5198, Val Loss: 0.5412, Val Acc: 98.25%\n📘 Learning rate reduced from 0.000018 to 0.000014\n⚠️ No improvement in validation loss for 119 epochs.\nEpoch [173/300], Train Loss: 0.5192, Val Loss: 0.5424, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 120 epochs.\nEpoch [174/300], Train Loss: 0.5191, Val Loss: 0.5418, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 121 epochs.\nEpoch [175/300], Train Loss: 0.5196, Val Loss: 0.5412, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 122 epochs.\nEpoch [176/300], Train Loss: 0.5190, Val Loss: 0.5418, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 123 epochs.\nEpoch [177/300], Train Loss: 0.5190, Val Loss: 0.5416, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 124 epochs.\nEpoch [178/300], Train Loss: 0.5196, Val Loss: 0.5408, Val Acc: 98.19%\n📘 Learning rate reduced from 0.000014 to 0.000012\n⚠️ No improvement in validation loss for 125 epochs.\nEpoch [179/300], Train Loss: 0.5191, Val Loss: 0.5407, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 126 epochs.\nEpoch [180/300], Train Loss: 0.5195, Val Loss: 0.5403, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 127 epochs.\nEpoch [181/300], Train Loss: 0.5192, Val Loss: 0.5408, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 128 epochs.\nEpoch [182/300], Train Loss: 0.5187, Val Loss: 0.5407, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 129 epochs.\nEpoch [183/300], Train Loss: 0.5186, Val Loss: 0.5399, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 130 epochs.\nEpoch [184/300], Train Loss: 0.5186, Val Loss: 0.5404, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 131 epochs.\nEpoch [185/300], Train Loss: 0.5189, Val Loss: 0.5413, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 132 epochs.\nEpoch [186/300], Train Loss: 0.5188, Val Loss: 0.5413, Val Acc: 98.25%\n✅ Epoch 187: Validation loss improved to 0.5404. Model saved!\nEpoch [187/300], Train Loss: 0.5194, Val Loss: 0.5404, Val Acc: 98.36%\n⚠️ No improvement in validation loss for 1 epochs.\nEpoch [188/300], Train Loss: 0.5182, Val Loss: 0.5409, Val Acc: 98.26%\n📘 Learning rate reduced from 0.000012 to 0.000009\n⚠️ No improvement in validation loss for 2 epochs.\nEpoch [189/300], Train Loss: 0.5189, Val Loss: 0.5415, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 3 epochs.\nEpoch [190/300], Train Loss: 0.5183, Val Loss: 0.5404, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 4 epochs.\nEpoch [191/300], Train Loss: 0.5178, Val Loss: 0.5404, Val Acc: 98.32%\n⚠️ No improvement in validation loss for 5 epochs.\nEpoch [192/300], Train Loss: 0.5188, Val Loss: 0.5405, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 6 epochs.\nEpoch [193/300], Train Loss: 0.5184, Val Loss: 0.5409, Val Acc: 98.36%\n⚠️ No improvement in validation loss for 7 epochs.\nEpoch [194/300], Train Loss: 0.5184, Val Loss: 0.5412, Val Acc: 98.24%\n📘 Learning rate reduced from 0.000009 to 0.000007\n⚠️ No improvement in validation loss for 8 epochs.\nEpoch [195/300], Train Loss: 0.5188, Val Loss: 0.5408, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 9 epochs.\nEpoch [196/300], Train Loss: 0.5187, Val Loss: 0.5411, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 10 epochs.\nEpoch [197/300], Train Loss: 0.5184, Val Loss: 0.5407, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 11 epochs.\nEpoch [198/300], Train Loss: 0.5180, Val Loss: 0.5410, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 12 epochs.\nEpoch [199/300], Train Loss: 0.5188, Val Loss: 0.5407, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 13 epochs.\nEpoch [200/300], Train Loss: 0.5181, Val Loss: 0.5408, Val Acc: 98.26%\n📘 Learning rate reduced from 0.000007 to 0.000006\n⚠️ No improvement in validation loss for 14 epochs.\nEpoch [201/300], Train Loss: 0.5194, Val Loss: 0.5412, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 15 epochs.\nEpoch [202/300], Train Loss: 0.5181, Val Loss: 0.5413, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 16 epochs.\nEpoch [203/300], Train Loss: 0.5182, Val Loss: 0.5406, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 17 epochs.\nEpoch [204/300], Train Loss: 0.5184, Val Loss: 0.5410, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 18 epochs.\nEpoch [205/300], Train Loss: 0.5186, Val Loss: 0.5408, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 19 epochs.\nEpoch [206/300], Train Loss: 0.5182, Val Loss: 0.5420, Val Acc: 98.19%\n📘 Learning rate reduced from 0.000006 to 0.000005\n⚠️ No improvement in validation loss for 20 epochs.\nEpoch [207/300], Train Loss: 0.5189, Val Loss: 0.5418, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 21 epochs.\nEpoch [208/300], Train Loss: 0.5187, Val Loss: 0.5407, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 22 epochs.\nEpoch [209/300], Train Loss: 0.5187, Val Loss: 0.5405, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 23 epochs.\nEpoch [210/300], Train Loss: 0.5183, Val Loss: 0.5411, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 24 epochs.\nEpoch [211/300], Train Loss: 0.5188, Val Loss: 0.5419, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 25 epochs.\nEpoch [212/300], Train Loss: 0.5180, Val Loss: 0.5415, Val Acc: 98.19%\n📘 Learning rate reduced from 0.000005 to 0.000004\n⚠️ No improvement in validation loss for 26 epochs.\nEpoch [213/300], Train Loss: 0.5187, Val Loss: 0.5407, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 27 epochs.\nEpoch [214/300], Train Loss: 0.5175, Val Loss: 0.5409, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 28 epochs.\nEpoch [215/300], Train Loss: 0.5180, Val Loss: 0.5412, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 29 epochs.\nEpoch [216/300], Train Loss: 0.5185, Val Loss: 0.5408, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 30 epochs.\nEpoch [217/300], Train Loss: 0.5176, Val Loss: 0.5404, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 31 epochs.\nEpoch [218/300], Train Loss: 0.5184, Val Loss: 0.5411, Val Acc: 98.32%\n📘 Learning rate reduced from 0.000004 to 0.000003\n⚠️ No improvement in validation loss for 32 epochs.\nEpoch [219/300], Train Loss: 0.5177, Val Loss: 0.5408, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 33 epochs.\nEpoch [220/300], Train Loss: 0.5178, Val Loss: 0.5403, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 34 epochs.\nEpoch [221/300], Train Loss: 0.5179, Val Loss: 0.5408, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 35 epochs.\nEpoch [222/300], Train Loss: 0.5183, Val Loss: 0.5407, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 36 epochs.\nEpoch [223/300], Train Loss: 0.5183, Val Loss: 0.5417, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 37 epochs.\nEpoch [224/300], Train Loss: 0.5188, Val Loss: 0.5409, Val Acc: 98.24%\n📘 Learning rate reduced from 0.000003 to 0.000002\n⚠️ No improvement in validation loss for 38 epochs.\nEpoch [225/300], Train Loss: 0.5178, Val Loss: 0.5408, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 39 epochs.\nEpoch [226/300], Train Loss: 0.5188, Val Loss: 0.5415, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 40 epochs.\nEpoch [227/300], Train Loss: 0.5179, Val Loss: 0.5409, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 41 epochs.\nEpoch [228/300], Train Loss: 0.5179, Val Loss: 0.5409, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 42 epochs.\nEpoch [229/300], Train Loss: 0.5182, Val Loss: 0.5407, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 43 epochs.\nEpoch [230/300], Train Loss: 0.5177, Val Loss: 0.5410, Val Acc: 98.19%\n📘 Learning rate reduced from 0.000002 to 0.000002\n⚠️ No improvement in validation loss for 44 epochs.\nEpoch [231/300], Train Loss: 0.5188, Val Loss: 0.5419, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 45 epochs.\nEpoch [232/300], Train Loss: 0.5185, Val Loss: 0.5407, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 46 epochs.\nEpoch [233/300], Train Loss: 0.5182, Val Loss: 0.5405, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 47 epochs.\nEpoch [234/300], Train Loss: 0.5184, Val Loss: 0.5411, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 48 epochs.\nEpoch [235/300], Train Loss: 0.5177, Val Loss: 0.5412, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 49 epochs.\nEpoch [236/300], Train Loss: 0.5178, Val Loss: 0.5404, Val Acc: 98.27%\n📘 Learning rate reduced from 0.000002 to 0.000002\n⚠️ No improvement in validation loss for 50 epochs.\nEpoch [237/300], Train Loss: 0.5177, Val Loss: 0.5402, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 51 epochs.\nEpoch [238/300], Train Loss: 0.5185, Val Loss: 0.5407, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 52 epochs.\nEpoch [239/300], Train Loss: 0.5179, Val Loss: 0.5404, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 53 epochs.\nEpoch [240/300], Train Loss: 0.5178, Val Loss: 0.5414, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 54 epochs.\nEpoch [241/300], Train Loss: 0.5180, Val Loss: 0.5401, Val Acc: 98.32%\n⚠️ No improvement in validation loss for 55 epochs.\nEpoch [242/300], Train Loss: 0.5176, Val Loss: 0.5406, Val Acc: 98.27%\n📘 Learning rate reduced from 0.000002 to 0.000001\n⚠️ No improvement in validation loss for 56 epochs.\nEpoch [243/300], Train Loss: 0.5182, Val Loss: 0.5406, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 57 epochs.\nEpoch [244/300], Train Loss: 0.5180, Val Loss: 0.5408, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 58 epochs.\nEpoch [245/300], Train Loss: 0.5180, Val Loss: 0.5420, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 59 epochs.\nEpoch [246/300], Train Loss: 0.5175, Val Loss: 0.5418, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 60 epochs.\nEpoch [247/300], Train Loss: 0.5177, Val Loss: 0.5407, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 61 epochs.\nEpoch [248/300], Train Loss: 0.5182, Val Loss: 0.5407, Val Acc: 98.18%\n📘 Learning rate reduced from 0.000001 to 0.000001\n⚠️ No improvement in validation loss for 62 epochs.\nEpoch [249/300], Train Loss: 0.5178, Val Loss: 0.5410, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 63 epochs.\nEpoch [250/300], Train Loss: 0.5178, Val Loss: 0.5409, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 64 epochs.\nEpoch [251/300], Train Loss: 0.5175, Val Loss: 0.5404, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 65 epochs.\nEpoch [252/300], Train Loss: 0.5186, Val Loss: 0.5413, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 66 epochs.\nEpoch [253/300], Train Loss: 0.5176, Val Loss: 0.5407, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 67 epochs.\nEpoch [254/300], Train Loss: 0.5178, Val Loss: 0.5410, Val Acc: 98.18%\n📘 Learning rate reduced from 0.000001 to 0.000001\n⚠️ No improvement in validation loss for 68 epochs.\nEpoch [255/300], Train Loss: 0.5175, Val Loss: 0.5404, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 69 epochs.\nEpoch [256/300], Train Loss: 0.5180, Val Loss: 0.5419, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 70 epochs.\nEpoch [257/300], Train Loss: 0.5183, Val Loss: 0.5410, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 71 epochs.\nEpoch [258/300], Train Loss: 0.5179, Val Loss: 0.5409, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 72 epochs.\nEpoch [259/300], Train Loss: 0.5176, Val Loss: 0.5410, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 73 epochs.\nEpoch [260/300], Train Loss: 0.5179, Val Loss: 0.5409, Val Acc: 98.26%\n📘 Learning rate reduced from 0.000001 to 0.000001\n⚠️ No improvement in validation loss for 74 epochs.\nEpoch [261/300], Train Loss: 0.5179, Val Loss: 0.5411, Val Acc: 98.24%\n⚠️ No improvement in validation loss for 75 epochs.\nEpoch [262/300], Train Loss: 0.5184, Val Loss: 0.5415, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 76 epochs.\nEpoch [263/300], Train Loss: 0.5180, Val Loss: 0.5408, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 77 epochs.\nEpoch [264/300], Train Loss: 0.5180, Val Loss: 0.5409, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 78 epochs.\nEpoch [265/300], Train Loss: 0.5184, Val Loss: 0.5403, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 79 epochs.\nEpoch [266/300], Train Loss: 0.5180, Val Loss: 0.5408, Val Acc: 98.27%\n📘 Learning rate reduced from 0.000001 to 0.000001\n⚠️ No improvement in validation loss for 80 epochs.\nEpoch [267/300], Train Loss: 0.5176, Val Loss: 0.5411, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 81 epochs.\nEpoch [268/300], Train Loss: 0.5182, Val Loss: 0.5406, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 82 epochs.\nEpoch [269/300], Train Loss: 0.5177, Val Loss: 0.5406, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 83 epochs.\nEpoch [270/300], Train Loss: 0.5175, Val Loss: 0.5407, Val Acc: 98.32%\n⚠️ No improvement in validation loss for 84 epochs.\nEpoch [271/300], Train Loss: 0.5175, Val Loss: 0.5407, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 85 epochs.\nEpoch [272/300], Train Loss: 0.5184, Val Loss: 0.5406, Val Acc: 98.27%\n📘 Learning rate reduced from 0.000001 to 0.000000\n⚠️ No improvement in validation loss for 86 epochs.\nEpoch [273/300], Train Loss: 0.5177, Val Loss: 0.5407, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 87 epochs.\nEpoch [274/300], Train Loss: 0.5178, Val Loss: 0.5400, Val Acc: 98.31%\n⚠️ No improvement in validation loss for 88 epochs.\nEpoch [275/300], Train Loss: 0.5180, Val Loss: 0.5403, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 89 epochs.\nEpoch [276/300], Train Loss: 0.5179, Val Loss: 0.5410, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 90 epochs.\nEpoch [277/300], Train Loss: 0.5179, Val Loss: 0.5408, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 91 epochs.\nEpoch [278/300], Train Loss: 0.5174, Val Loss: 0.5410, Val Acc: 98.29%\n📘 Learning rate reduced from 0.000000 to 0.000000\n⚠️ No improvement in validation loss for 92 epochs.\nEpoch [279/300], Train Loss: 0.5184, Val Loss: 0.5408, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 93 epochs.\nEpoch [280/300], Train Loss: 0.5184, Val Loss: 0.5416, Val Acc: 98.17%\n⚠️ No improvement in validation loss for 94 epochs.\nEpoch [281/300], Train Loss: 0.5182, Val Loss: 0.5416, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 95 epochs.\nEpoch [282/300], Train Loss: 0.5185, Val Loss: 0.5407, Val Acc: 98.27%\n⚠️ No improvement in validation loss for 96 epochs.\nEpoch [283/300], Train Loss: 0.5174, Val Loss: 0.5406, Val Acc: 98.18%\n⚠️ No improvement in validation loss for 97 epochs.\nEpoch [284/300], Train Loss: 0.5179, Val Loss: 0.5405, Val Acc: 98.20%\n📘 Learning rate reduced from 0.000000 to 0.000000\n⚠️ No improvement in validation loss for 98 epochs.\nEpoch [285/300], Train Loss: 0.5177, Val Loss: 0.5403, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 99 epochs.\nEpoch [286/300], Train Loss: 0.5179, Val Loss: 0.5409, Val Acc: 98.19%\n⚠️ No improvement in validation loss for 100 epochs.\nEpoch [287/300], Train Loss: 0.5176, Val Loss: 0.5405, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 101 epochs.\nEpoch [288/300], Train Loss: 0.5180, Val Loss: 0.5412, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 102 epochs.\nEpoch [289/300], Train Loss: 0.5181, Val Loss: 0.5408, Val Acc: 98.29%\n⚠️ No improvement in validation loss for 103 epochs.\nEpoch [290/300], Train Loss: 0.5182, Val Loss: 0.5408, Val Acc: 98.33%\n📘 Learning rate reduced from 0.000000 to 0.000000\n⚠️ No improvement in validation loss for 104 epochs.\nEpoch [291/300], Train Loss: 0.5187, Val Loss: 0.5410, Val Acc: 98.21%\n⚠️ No improvement in validation loss for 105 epochs.\nEpoch [292/300], Train Loss: 0.5174, Val Loss: 0.5406, Val Acc: 98.26%\n⚠️ No improvement in validation loss for 106 epochs.\nEpoch [293/300], Train Loss: 0.5179, Val Loss: 0.5411, Val Acc: 98.20%\n⚠️ No improvement in validation loss for 107 epochs.\nEpoch [294/300], Train Loss: 0.5182, Val Loss: 0.5410, Val Acc: 98.30%\n⚠️ No improvement in validation loss for 108 epochs.\nEpoch [295/300], Train Loss: 0.5178, Val Loss: 0.5414, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 109 epochs.\nEpoch [296/300], Train Loss: 0.5184, Val Loss: 0.5403, Val Acc: 98.24%\n📘 Learning rate reduced from 0.000000 to 0.000000\n⚠️ No improvement in validation loss for 110 epochs.\nEpoch [297/300], Train Loss: 0.5181, Val Loss: 0.5403, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 111 epochs.\nEpoch [298/300], Train Loss: 0.5175, Val Loss: 0.5405, Val Acc: 98.23%\n⚠️ No improvement in validation loss for 112 epochs.\nEpoch [299/300], Train Loss: 0.5177, Val Loss: 0.5407, Val Acc: 98.25%\n⚠️ No improvement in validation loss for 113 epochs.\nEpoch [300/300], Train Loss: 0.5182, Val Loss: 0.5411, Val Acc: 98.23%\n🎯 Best model loaded with Val Loss: 0.5404\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-12-21994229cdcc>:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Model 2 with skip connection","metadata":{}},{"cell_type":"code","source":"# Define the Residual Block with Skip Connection\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features, activation=nn.SiLU(), dropout_rate=0.3):\n        \"\"\"\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        activation (nn.Module): Activation function to use (default: SiLU).\n        dropout_rate (float): Dropout rate (default: 0.3).\n        \"\"\"\n        super(ResidualBlock, self).__init__()\n        \n        # Main layers\n        self.linear = nn.Linear(in_features, out_features)\n        self.bn = nn.BatchNorm1d(out_features)\n        self.activation = activation\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Skip connection: If input and output dimensions don't match, use a linear layer to adjust dimensions\n        self.skip = nn.Linear(in_features, out_features) if in_features != out_features else None\n\n    def forward(self, x):\n        identity = x  # Save the input for the skip connection\n        out = self.linear(x)\n        out = self.bn(out)\n        out = self.activation(out)\n        out = self.dropout(out)\n        \n        # Apply skip connection\n        if self.skip is not None:\n            identity = self.skip(identity)\n        out += identity  # Add the skip connection\n        return out\n\n# Define the MLP Model with Residual Blocks\nclass MLPModel(nn.Module):\n    def __init__(self, input_size=784, hidden_size=[512, 256, 128, 64], output_size=10):\n        super(MLPModel, self).__init__()\n        self.layers = nn.ModuleList([\n            # ResidualBlock(input_size, hidden_size[0],nn.SiLU(),0.3),  # First residual block\n            # ResidualBlock(hidden_size[0], hidden_size[1], nn.LeakyReLU(negative_slope=0.03), 0.3),  # Second residual block\n            # ResidualBlock(hidden_size[1], hidden_size[2], nn.Tanh(), 0.2),  # Third residual block\n            # ResidualBlock(hidden_size[2], hidden_size[3], nn.Mish(), 0.2),  # Fourth residual block\n            # nn.Linear(hidden_size[3], output_size)  # Final output layer\n\n            \n            nn.Linear(input_size, hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            nn.SiLU(),\n            nn.Dropout(0.3),\n           \n            nn.Linear(hidden_size[0], hidden_size[1]),\n            nn.BatchNorm1d(hidden_size[1]),\n            # nn.ReLU(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            nn.Tanh(),\n            # nn.Sigmoid(),\n            nn.Dropout(0.3),\n\n            ResidualBlock(hidden_size[1], hidden_size[2], nn.LeakyReLU(negative_slope=0.03),0.2)  # Third residual block          \n\n            nn.Linear(hidden_size[2], hidden_size[3]),\n            nn.BatchNorm1d(hidden_size[3]),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(hidden_size[3], output_size)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Preprocess data (example)\nX = train_df.iloc[:, 1:].values / 255.0  # Normalize pixel values\ny = train_df.iloc[:, 0].values  # Labels\n\n# Train-validation split (80-20)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n\n# Initialize model, loss function, and optimizer\nmodel = MLPModel().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.RAdam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.8, patience=10)\n\n# Training loop with best model saving\nnum_epochs = 300\npatience = 200  # Number of epochs to wait before stopping\nbest_val_loss = float(\"inf\")  # Start with a very high loss\nbest_val_acc = 0.0\nmodel_path = \"best_mlp_digit_recognizer.pth\"\nepochs_no_improve = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    avg_train_loss = running_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n\n    current_lr = optimizer.param_groups[0]['lr']\n    scheduler.step(val_accuracy)\n    new_lr = optimizer.param_groups[0]['lr']\n    if new_lr < current_lr:\n        print(f\"📘 Learning rate reduced from {current_lr:.6f} to {new_lr:.6f}\")\n    \n    # Check if validation loss improved\n    if val_accuracy > best_val_acc:\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy \n        torch.save(model.state_dict(), model_path)\n        print(f\"✅ Epoch {epoch+1}: Validation loss improved to {best_val_loss:.4f}. Model saved!\")\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        print(f\"⚠️ No improvement in validation loss for {epochs_no_improve} epochs.\")\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n    # Stop training if no improvement for 'patience' epochs\n    if epochs_no_improve >= patience:\n        print(\"🚨 Early stopping triggered! Loading best model...\")\n        break\n\n# Load the best model\nmodel.load_state_dict(torch.load(model_path))\nprint(f\"🎯 Best model loaded with Val Loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T12:10:25.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Nháp","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n# Preprocess data\nX = train_df.iloc[:, 1:].values / 255.0  # Normalize pixel values\ny = train_df.iloc[:, 0].values           # Labels\n\n# Train-validation split (80-20)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n\n# Define the MLP model using nn.Sequential\nclass MLPModel(nn.Module):\n    def __init__(self, input_size=784, hidden_size=[512,256,128,64], output_size=10):\n        super(MLPModel, self).__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(input_size, hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            # nn.SiLU(),\n            nn.Tanh(),\n            nn.Dropout(0.3),\n           \n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            # nn.Tanh(),\n            nn.Sigmoid(),\n            nn.Dropout(0.3),\n\n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            nn.Tanh(),\n            # nn.Sigmoid(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(hidden_size[0], hidden_size[0]),\n            nn.BatchNorm1d(hidden_size[0]),\n            # nn.ReLU(),\n            # nn.Mish(),\n            # nn.LeakyReLU(negative_slope=0.03),\n            # nn.Tanh(),\n            nn.Sigmoid(),\n            nn.Dropout(0.4),\n            \n            nn.Linear(hidden_size[0], output_size)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialize model, loss function, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MLPModel().to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n# optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\noptimizer = optim.RAdam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-4)\n# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=True)\n\n\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=num_epochs)\n\n# Training loop with best model saving\nnum_epochs = 300\npatience = 200  # Number of epochs to wait before stopping\nbest_val_loss = float(\"inf\")  # Start with a very high loss\nbest_val_acc = 0.0\nmodel_path = \"best_mlp_digit_recognizer.pth\"\nepochs_no_improve = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    avg_train_loss = running_loss / len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n\n    current_lr = optimizer.param_groups[0]['lr']\n    scheduler.step(avg_val_loss)\n    new_lr = optimizer.param_groups[0]['lr']\n    if new_lr < current_lr:\n        print(f\"📘 Learning rate reduced from {current_lr:.6f} to {new_lr:.6f}\")\n    \n    # Check if validation loss improved\n    if val_accuracy > best_val_acc:\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy \n        torch.save(model.state_dict(), model_path)\n        print(f\"✅ Epoch {epoch+1}: Validation loss improved to {best_val_loss:.4f}. Model saved!\")\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        print(f\"⚠️ No improvement in validation loss for {epochs_no_improve} epochs.\")\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n    # Stop training if no improvement for 'patience' epochs\n    if epochs_no_improve >= patience:\n        print(\"🚨 Early stopping triggered! Loading best model...\")\n        break\n\n# Load the best model\nmodel.load_state_dict(torch.load(model_path))\nprint(f\"🎯 Best model loaded with Val Loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T12:10:25.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"best_mlp_digit_recognizer.pth\")\n# torch.save(model.state_dict(), \"mlp_digit_recognizer.pth\")\n# torch.save(model.state_dict(), \"a1.pth\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T12:10:25.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MLPModel().to(device)  # Recreate the model structure\n# Load the best model for final evaluation\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\n\n# =======================\n# Validate the model\n# =======================\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n\n# =======================\n# Test the model on test.csv\n# =======================\nX_test = test_df.values / 255.0  # Normalize test images\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n\nwith torch.no_grad():\n    test_outputs = model(X_test_tensor)\n    predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()  # Convert to numpy array\n\n# Save predictions to submission.csv (for Kaggle)\nsubmission = pd.DataFrame({\"ImageId\": np.arange(1, len(predictions) + 1), \"Label\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Predictions saved to submission.csv!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T12:10:25.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}